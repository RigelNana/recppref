{{cpp/algorithm/execution/title|sequenced_policy|parallel_policy|parallel_unsequenced_policy|unsequenced_policy}}
{{cpp/algorithm/navbar}}

{{dcl begin}}
{{dcl header|execution}}
{{dcl|num=1|since=c++17|
class sequenced_policy { /* unspecified */ };
}}
{{dcl|num=2|since=c++17|
class parallel_policy { /* unspecified */ };
}}
{{dcl|num=3|since=c++17|
class parallel_unsequenced_policy { /* unspecified */ };
}}
{{dcl|num=4|since=c++20|
class unsequenced_policy { /* unspecified */ };
}}
{{dcl end}}

@1@ The execution policy type used as a unique type to disambiguate parallel algorithm overloading and require that a parallel algorithm's execution may not be parallelized. The invocations of element access functions in parallel algorithms invoked with this policy (usually specified as {{lc|std::execution::seq}}) are indeterminately sequenced in the calling thread.
@2@ The execution policy type used as a unique type to disambiguate parallel algorithm overloading and indicate that a parallel algorithm's execution may be parallelized. The invocations of element access functions in parallel algorithms invoked with this policy (usually specified as {{lc|std::execution::par}}) are permitted to execute in either the invoking thread or in a thread implicitly created by the library to support parallel algorithm execution. Any such invocations executing in the same thread are indeterminately sequenced with respect to each other. If the threads of execution created by {{lc|std::thread}} or {{lc|std::jthread}} provide concurrent forward progress guarantees, then the threads of execution created by the library provide parallel forward progress guarantees. Otherwise, the provided forward progress guarantee is implementation-defined. Note: parallel forward progress guarantees that if a thread of execution makes a step, it will eventually make another step, enabling threads to enter critical sections and take locks, because the thread that has the lock will be eventually scheduled again and be able to release it.
@3@ The execution policy type used as a unique type to disambiguate parallel algorithm overloading and indicate that a parallel algorithm's execution may be parallelized, vectorized, or migrated across threads (such as by a parent-stealing scheduler). The invocations of element access functions in parallel algorithms invoked with this policy are permitted to execute in an unordered fashion in unspecified threads, and unsequenced with respect to one another within each thread. The invocation of element access functions in parallel algorithms invoked with this policy are not permitted to invoke vectorization-unsafe operations, like those specified by the standard library to synchronize, including those of {{lc|std::atomic}} and other concurrency primitives. If the threads of execution created by {{lc|std::thread}} or {{lc|std::jthread}} provide concurrent forward progress guarantees, then the threads of execution created by the library provide weakly parallel forward progress guarantees. Otherwise, the provided forward progress guarantee is that of the thread invoking the parallel algorithm. Note: weakly parallel forward progress guarantees that one of the threads of execution that have made a step will eventually make another step, which does not allow threads to enter critical sections or take locks, because the thread that has the lock may not be scheduled again until a thread that is attempting to take the lock has exited.
@4@ The execution policy type used as a unique type to disambiguate parallel algorithm overloading and indicate that a parallel algorithm's execution may be vectorized, e.g., executed on a single thread using instructions that operate on multiple data items.

During the execution of a parallel algorithm with any of these execution policies, if the invocation of an element access function exits via an uncaught exception, {{lc|std::terminate}} is called, but the implementations may define additional execution policies that handle exceptions differently.

===Notes===
When using parallel execution policy, it is the programmer's responsibility to avoid data races and deadlocks:
{{source|1=
int a[] = {0, 1};
std::vector&lt;int&gt; v;
std::for_each(std::execution::par, std::begin(a), std::end(a), [&amp;](int i)
{
    v.push_back(i * 2 + 1); // Error: data race
});
}}

{{source|1=
std::atomic&lt;int&gt; x {0};
int a[] = {1, 2};
std::for_each(std::execution::par, std::begin(a), std::end(a), [&amp;](int)
{
    x.fetch_add(1, std::memory_order_relaxed);
    while (x.load(std::memory_order_relaxed) == 1) { } // Error: assumes execution order
});
}}

{{source|1=
int x = 0;
std::mutex m;
int a[] = {1, 2};
std::for_each(std::execution::par, std::begin(a), std::end(a), [&amp;](int)
{
    std::lock_guard&lt;std::mutex&gt; guard(m);
    ++x; // correct
});
}}

Unsequenced execution policies are the only case where function calls are ''unsequenced'' with respect to each other, meaning they can be interleaved. In all other situations in C++, they are [[cpp/language/eval_order|indeterminately-sequenced]] (cannot interleave). Because of that, users are not allowed to allocate or deallocate memory, acquire mutexes, use non-lockfree {{lc|std::atomic}} specializations, or, in general, perform any ''vectorization-unsafe'' operations when using these policies (vectorization-unsafe functions are the ones that synchronize-with another function, e.g. {{lc|std::mutex::unlock}} synchronizes-with the next {{lc|std::mutex::lock}}).
{{source|1=
int x = 0;
std::mutex m;
int a[] = {1, 2};
std::for_each(std::execution::par_unseq, std::begin(a), std::end(a), [&amp;](int)
{
    std::lock_guard&lt;std::mutex&gt; guard(m); // Error: lock_guard constructor calls m.lock()
    ++x;
});
}}

If the implementation cannot parallelize or vectorize (e.g. due to lack of resources), all standard execution policies can fall back to sequential execution.

===See also===
{{dsc begin}}
{{dsc inc|cpp/algorithm/dsc execution_policy_tag}}
{{dsc end}}

{{langlinks|es|ja|ru|zh}}